{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TF2 Object Detection from Checkpoint Efficientdet - construction tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tobieabel/robokeeper/blob/master/TF2_Object_Detection_from_Checkpoint_Efficientdet_construction_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wR7fNU6nTDcJ"
      },
      "source": [
        "## TF2 OD API - training tutorial\n",
        "\n",
        "\n",
        "In this Colab notebook, we show how to train (fine-tune) a pre-trained model that is  available in [TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md). The model is trained on construction safety equipment dataset that is given as courtesy of Forsight.\n",
        "\n",
        "We are also open-sourcing our train/eval script that enables you to simultaneously run and evaluate models without having to manually stop and reset the training.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3oHKsyCB9Jc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a50e9c6-38d0-410f-b851-c954a3fed953"
      },
      "source": [
        "# Depending on your hardware specs install either tensorflow (CPU) or tensorflow_gpu (GPU) version\n",
        "\n",
        "# In this notebook, we use tensorflow_gpu. Make sure that GPU hardware acceleration is enabled in your Google Colab notebook\n",
        "# by checking \"Runtime -> Change runtime type\" and selecting \"GPU accelerator\"\n",
        "!pip install tensorflow-gpu\n",
        "\n",
        "# Install Trains open-source experiment management tool that will help us in experiment management.\n",
        "!pip install trains"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/6d/67169e8d8146f377bbfd71d6c108a0fce218411371ce41d440a7a5f5fb20/tensorflow_gpu-2.4.1-cp36-cp36m-manylinux2010_x86_64.whl (394.3MB)\n",
            "\u001b[K     |████████████████████████████████| 394.3MB 41kB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.3.3)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.10.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.7.4.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.4.1)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.12.4)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.32.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.4.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.36.2)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (51.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (0.4.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (1.17.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (3.3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.7)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu) (3.4.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-2.4.1\n",
            "Collecting trains\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/e4/9b283e7ed1ac1c9bb36e362ce6cbca6f604be000263f81f67c99aa5c34c5/trains-0.16.4-py2.py3-none-any.whl (855kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 4.1MB/s \n",
            "\u001b[?25hCollecting pyjwt>=1.6.4\n",
            "  Downloading https://files.pythonhosted.org/packages/b4/9b/8850f99027ed029af6828199cc87179eaccbbf1f9e6e373e7f0177d32dad/PyJWT-2.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from trains) (2.23.0)\n",
            "Requirement already satisfied: Pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from trains) (7.0.0)\n",
            "Collecting funcsigs>=1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from trains) (2.8.1)\n",
            "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.6/dist-packages (from trains) (3.13)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from trains) (1.15.0)\n",
            "Collecting pathlib2>=2.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/45/9c82d3666af4ef9f221cbb954e1d77ddbb513faf552aea6df5f37f1a4859/pathlib2-2.3.5-py2.py3-none-any.whl\n",
            "Collecting humanfriendly>=2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/66/363d01a81da2108a5cf446daf619779f06d49a0c4426dd02b40734f10e2f/humanfriendly-9.1-py2.py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.7MB/s \n",
            "\u001b[?25hCollecting requests-file>=1.4.2\n",
            "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from trains) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from trains) (2.4.7)\n",
            "Requirement already satisfied: psutil>=3.4.2 in /usr/local/lib/python3.6/dist-packages (from trains) (5.4.8)\n",
            "Requirement already satisfied: jsonschema>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from trains) (2.6.0)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.6/dist-packages (from trains) (1.19.5)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from trains) (0.16.0)\n",
            "Requirement already satisfied: attrs>=18.0 in /usr/local/lib/python3.6/dist-packages (from trains) (20.3.0)\n",
            "Collecting furl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/9f/c7/e9dc30914bf048bcd06284bb93d9650d318ecac8668b684fc41e975558ff/furl-2.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->trains) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->trains) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->trains) (2020.12.5)\n",
            "Collecting orderedmultidict>=1.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/04/16/5e95c70bda8fe6ea715005c0db8e602400bdba50ae3c72cb380eba551289/orderedmultidict-1.0.1-py2.py3-none-any.whl\n",
            "Installing collected packages: pyjwt, funcsigs, pathlib2, humanfriendly, requests-file, orderedmultidict, furl, trains\n",
            "Successfully installed funcsigs-1.0.2 furl-2.1.0 humanfriendly-9.1 orderedmultidict-1.0.1 pathlib2-2.3.5 pyjwt-2.0.1 requests-file-1.5.1 trains-0.16.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7wVe_nICeWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d31de00c-628e-49ec-c898-4afd09a9d658"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "# Clone the tensorflow models repository if it doesn't already exist.\n",
        "# We clone from a forked repo because the official tf repo doesn't support per object\n",
        "# statistics in eval, and it doesn't support simultaneous train and eval. \n",
        "# Take a look into this issue on TF OD API git for additional details:\n",
        "# https://github.com/tensorflow/models/issues/4778#issuecomment-430262110\n",
        "\n",
        "if \"models\" in pathlib.Path.cwd().parts:\n",
        "  while \"models\" in pathlib.Path.cwd().parts:\n",
        "    os.chdir('..')\n",
        "elif not pathlib.Path('models').exists():\n",
        "  !git clone --depth 1 https://github.com/qraleq/models\n",
        "\n",
        "# By default the cloned repository is located in /content/models/\n",
        "# The TF OD API part of the repo is located in /content/models/research/object_detection/\n",
        "# Usually, all the scripts are ran from /content/models/research/ as the base directory\n",
        "# In order to explore the scripts in the OD API, you can use the Files navigation bar\n",
        "# on the left side of the Google Colab notebook.\n",
        "# Double-clicking a file will open a scratch editor on the right side of the Google Colab notebook.\n",
        "\n",
        "%cd /content/models/research\n",
        "\n",
        "# COCO is a large image dataset designed for object detection, segmentation, person keypoints detection, stuff segmentation, and caption generation. \n",
        "# TF 2 OD API uses PythonAPI pycocotools for performing evaluation of the object detection models.\n",
        "# By default, pycocotools do not support per object statistic in eval, so we replace cocoeval.py script with a custom one that does support it.\n",
        "!cp ./object_detection/metrics/cocoeval.py /usr/local/lib/python3.6/dist-packages/pycocotools/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 1968, done.\u001b[K\n",
            "remote: Counting objects: 100% (1968/1968), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1686/1686), done.\u001b[K\n",
            "remote: Total 1968 (delta 471), reused 879 (delta 262), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1968/1968), 51.42 MiB | 16.74 MiB/s, done.\n",
            "Resolving deltas: 100% (471/471), done.\n",
            "/content/models/research\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-0MvJVuNMdD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a165d2cf-f9f6-43e7-ea92-1742b61aa20b"
      },
      "source": [
        "# Compile protos.\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "# Export OD API to Python path and install dependencies\n",
        "os.environ['PYTHONPATH'] += \":/content/models/research:/content/models/research/slim:/content/models/research/slim/nets:/content/models/research/object_detection\"\n",
        "!pip install tf_slim tf-models-official"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf_slim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\r\u001b[K     |█                               | 10kB 23.4MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20kB 9.8MB/s eta 0:00:01\r\u001b[K     |██▉                             | 30kB 7.8MB/s eta 0:00:01\r\u001b[K     |███▊                            | 40kB 7.2MB/s eta 0:00:01\r\u001b[K     |████▋                           | 51kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 61kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 81kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 92kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 102kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 112kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 122kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 133kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 143kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 153kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 163kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 174kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 184kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 194kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 204kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 215kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 225kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 235kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 245kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 256kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 266kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 276kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 286kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 296kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 307kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 317kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 327kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 337kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 348kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 358kB 5.7MB/s \n",
            "\u001b[?25hCollecting tf-models-official\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/4a/23a08f8fd2747867ee223612e219eeb0d11c36116601d99b55ef3c72e707/tf_models_official-2.4.0-py2.py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 32.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from tf_slim) (0.10.0)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (0.8.3)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (4.1.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (0.8)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (3.2.2)\n",
            "Collecting opencv-python-headless\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/fc/4da675cc522a749ebbcf85c5a63fba844b2d44c87e6f24e3fdb147df3270/opencv_python_headless-4.5.1.48-cp36-cp36m-manylinux2014_x86_64.whl (37.6MB)\n",
            "\u001b[K     |████████████████████████████████| 37.6MB 84kB/s \n",
            "\u001b[?25hCollecting tensorflow-model-optimization>=0.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/38/4fd48ea1bfcb0b6e36d949025200426fe9c3a8bfae029f0973d85518fa5a/tensorflow_model_optimization-0.5.0-py2.py3-none-any.whl (172kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 60.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (1.5.10)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (1.1.5)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (5.4.8)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (0.29.21)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 58.3MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/5b/bc0b5ab38247bba158504a410112b6c03f153c652734ece1849749e5f518/PyYAML-5.4.1-cp36-cp36m-manylinux1_x86_64.whl (640kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 55.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (2.4.1)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (1.7.12)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (0.11.0)\n",
            "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (1.21.0)\n",
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (4.0.1)\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/f5/8e6e85ce2e9f6e05040cf0d4e26f43a4718bcc4bce988b433276d4b1a5c1/py-cpuinfo-7.0.0.tar.gz (95kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (1.19.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (7.0.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.6/dist-packages (from tf-models-official) (2.0.2)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons->tf-models-official) (2.7.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tf-models-official) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tf-models-official) (4.7)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tf-models-official) (0.2.8)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tf-models-official) (0.17.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-official) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-official) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-official) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-official) (2.4.7)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official) (0.1.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-official) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-official) (2.23.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-official) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-official) (2020.12.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-official) (4.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->tf-models-official) (2018.9)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0->tf-models-official) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0->tf-models-official) (3.3.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0->tf-models-official) (0.36.2)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0->tf-models-official) (1.32.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0->tf-models-official) (2.4.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0->tf-models-official) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0->tf-models-official) (3.12.4)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0->tf-models-official) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0->tf-models-official) (3.7.4.3)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0->tf-models-official) (2.4.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0->tf-models-official) (0.2.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0->tf-models-official) (1.6.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0->tf-models-official) (0.3.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0->tf-models-official) (1.12)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.4.0->tf-models-official) (2.10.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (1.17.2)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.0.4)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official) (1.0.3)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official) (0.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval->tf-models-official) (0.22.2.post1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official) (2.3)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official) (20.3.0)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official) (5.1.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official) (0.16.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official) (0.3.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official) (0.27.0)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools->tf-models-official) (51.3.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle>=1.3.9->tf-models-official) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle>=1.3.9->tf-models-official) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official) (1.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official) (0.4.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.6.7->tf-models-official) (4.2.1)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.0.3->google-cloud-bigquery>=0.31.0->tf-models-official) (1.16.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (1.0.0)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets->tf-models-official) (3.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->tf-models-official) (1.52.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official) (3.1.0)\n",
            "Building wheels for collected packages: seqeval, py-cpuinfo\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp36-none-any.whl size=16171 sha256=159ac1df98247032b8e6482b7b1eedaaabdbf472951eafa43500fc5a29337069\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-7.0.0-cp36-none-any.whl size=20072 sha256=0957180794eb83731aa8f62931b829d4d4018a034891e87e870ce356276fac41\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/93/7b/127daf0c3a5a49feb2fecd468d508067c733fba5192f726ad1\n",
            "Successfully built seqeval py-cpuinfo\n",
            "Installing collected packages: tf-slim, opencv-python-headless, tensorflow-model-optimization, sentencepiece, pyyaml, seqeval, py-cpuinfo, tf-models-official\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed opencv-python-headless-4.5.1.48 py-cpuinfo-7.0.0 pyyaml-5.4.1 sentencepiece-0.1.95 seqeval-1.2.2 tensorflow-model-optimization-0.5.0 tf-models-official-2.4.0 tf-slim-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "zwp4YuJNX3Kp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f4288b9-7620-4463-cd1c-7192fc971498"
      },
      "source": [
        "# Download EfficientDet D0 checkpoint and uncompress it\n",
        "%cd ./object_detection/models/checkpoints/detection/\n",
        "\n",
        "# We use a simple bash script that downloads and uncompresses the EfficientDet\n",
        "# checkpoint for us\n",
        "!bash download_d0_efficientdet_checkpoints.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/object_detection/models/checkpoints/detection\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 29.3M  100 29.3M    0     0  13.8M      0  0:00:02  0:00:02 --:--:-- 13.8M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4BHjz5kuzpI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7de57cf8-082b-4f8c-c3b0-d372cc8d1a5e"
      },
      "source": [
        "# Download and prepare the dataset and label map\n",
        "%cd /content\n",
        "!gdown --id 1GPU-RK-1GuCtbQzV6YQDRRX039HiQiBC\n",
        "!unzip tf_records.zip\n",
        "!ls tf_records"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1GPU-RK-1GuCtbQzV6YQDRRX039HiQiBC\n",
            "To: /content/tf_records.zip\n",
            "265MB [00:04, 54.2MB/s]\n",
            "Archive:  tf_records.zip\n",
            "   creating: tf_records/\n",
            "  inflating: tf_records/construction_label_map.pbtxt  \n",
            "  inflating: tf_records/test.rcd-00000-of-00010  \n",
            "  inflating: tf_records/test.rcd-00001-of-00010  \n",
            "  inflating: tf_records/test.rcd-00002-of-00010  \n",
            "  inflating: tf_records/test.rcd-00003-of-00010  \n",
            "  inflating: tf_records/test.rcd-00004-of-00010  \n",
            "  inflating: tf_records/test.rcd-00005-of-00010  \n",
            "  inflating: tf_records/test.rcd-00006-of-00010  \n",
            "  inflating: tf_records/test.rcd-00007-of-00010  \n",
            "  inflating: tf_records/test.rcd-00008-of-00010  \n",
            "  inflating: tf_records/test.rcd-00009-of-00010  \n",
            "  inflating: tf_records/train.rcd-00000-of-00010  \n",
            "  inflating: tf_records/train.rcd-00001-of-00010  \n",
            "  inflating: tf_records/train.rcd-00002-of-00010  \n",
            "  inflating: tf_records/train.rcd-00003-of-00010  \n",
            "  inflating: tf_records/train.rcd-00004-of-00010  \n",
            "  inflating: tf_records/train.rcd-00005-of-00010  \n",
            "  inflating: tf_records/train.rcd-00006-of-00010  \n",
            "  inflating: tf_records/train.rcd-00007-of-00010  \n",
            "  inflating: tf_records/train.rcd-00008-of-00010  \n",
            "  inflating: tf_records/train.rcd-00009-of-00010  \n",
            "construction_label_map.pbtxt  train.rcd-00000-of-00010\n",
            "test.rcd-00000-of-00010       train.rcd-00001-of-00010\n",
            "test.rcd-00001-of-00010       train.rcd-00002-of-00010\n",
            "test.rcd-00002-of-00010       train.rcd-00003-of-00010\n",
            "test.rcd-00003-of-00010       train.rcd-00004-of-00010\n",
            "test.rcd-00004-of-00010       train.rcd-00005-of-00010\n",
            "test.rcd-00005-of-00010       train.rcd-00006-of-00010\n",
            "test.rcd-00006-of-00010       train.rcd-00007-of-00010\n",
            "test.rcd-00007-of-00010       train.rcd-00008-of-00010\n",
            "test.rcd-00008-of-00010       train.rcd-00009-of-00010\n",
            "test.rcd-00009-of-00010\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20taX9u9rBq0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb3def18-86b9-48c2-c0a4-669fb01e6921"
      },
      "source": [
        "# Update the pipeline.config file for the EfficientDet checkpoint.\n",
        "%cd /content/models/research\n",
        "\n",
        "from object_detection.utils import config_util\n",
        "\n",
        "configs = config_util.get_configs_from_pipeline_file(f'./object_detection/models/checkpoints/detection/efficientdet_d0_coco17_tpu-32/pipeline.config')\n",
        "\n",
        "# Modify the model config\n",
        "model_config = configs['model'].ssd\n",
        "model_config.num_classes = 4\n",
        "\n",
        "train_config = configs['train_config']\n",
        "train_config.fine_tune_checkpoint = f'./object_detection/models/checkpoints/detection/efficientdet_d0_coco17_tpu-32/checkpoint/ckpt-0'\n",
        "train_config.fine_tune_checkpoint_type = 'detection'\n",
        "train_config.batch_size = 16\n",
        "\n",
        "train_input_reader_config = configs['train_input_config']\n",
        "train_input_reader_config.label_map_path = '/content/tf_records/construction_label_map.pbtxt'\n",
        "train_input_reader_config.tf_record_input_reader.input_path[:] = ['/content/tf_records/train.rcd-?????-of-?????']\n",
        "\n",
        "# Modify the eval_configs\n",
        "eval_config = configs['eval_config']\n",
        "eval_config.num_visualizations = 20 # total number of images for visualization\n",
        "eval_config.max_evals = 1 # total number of evals\n",
        "eval_config.include_metrics_per_category = True # include per category metrics\n",
        "eval_config.all_metrics_per_category = True # include detailed per category metrics\n",
        "\n",
        "# Modify the eval_input_configs\n",
        "eval_input_reader_config = configs['eval_input_config']\n",
        "eval_input_reader_config.label_map_path = '/content/tf_records/construction_label_map.pbtxt' # path to the label map\n",
        "eval_input_reader_config.tf_record_input_reader.input_path[:] = ['/content/tf_records/test.rcd-?????-of-?????']\n",
        "eval_input_reader_config.num_readers = 8\n",
        "\n",
        "# Convert config to pipeline proto and save to file\n",
        "pipeline_proto = config_util.create_pipeline_proto_from_configs(configs)\n",
        "config_util.save_pipeline_config(pipeline_config=pipeline_proto, directory=f'./object_detection/models/checkpoints/detection/efficientdet_d0_coco17_tpu-32/')\n",
        "!cat './object_detection/models/checkpoints/detection/efficientdet_d0_coco17_tpu-32/pipeline.config'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research\n",
            "INFO:tensorflow:Writing pipeline config file to ./object_detection/models/checkpoints/detection/efficientdet_d0_coco17_tpu-32/pipeline.config\n",
            "model {\n",
            "  ssd {\n",
            "    num_classes: 4\n",
            "    image_resizer {\n",
            "      keep_aspect_ratio_resizer {\n",
            "        min_dimension: 512\n",
            "        max_dimension: 512\n",
            "        pad_to_max_dimension: true\n",
            "      }\n",
            "    }\n",
            "    feature_extractor {\n",
            "      type: \"ssd_efficientnet-b0_bifpn_keras\"\n",
            "      conv_hyperparams {\n",
            "        regularizer {\n",
            "          l2_regularizer {\n",
            "            weight: 4e-05\n",
            "          }\n",
            "        }\n",
            "        initializer {\n",
            "          truncated_normal_initializer {\n",
            "            mean: 0.0\n",
            "            stddev: 0.03\n",
            "          }\n",
            "        }\n",
            "        activation: SWISH\n",
            "        batch_norm {\n",
            "          decay: 0.99\n",
            "          scale: true\n",
            "          epsilon: 0.001\n",
            "        }\n",
            "        force_use_bias: true\n",
            "      }\n",
            "      bifpn {\n",
            "        min_level: 3\n",
            "        max_level: 7\n",
            "        num_iterations: 3\n",
            "        num_filters: 64\n",
            "      }\n",
            "    }\n",
            "    box_coder {\n",
            "      faster_rcnn_box_coder {\n",
            "        y_scale: 1.0\n",
            "        x_scale: 1.0\n",
            "        height_scale: 1.0\n",
            "        width_scale: 1.0\n",
            "      }\n",
            "    }\n",
            "    matcher {\n",
            "      argmax_matcher {\n",
            "        matched_threshold: 0.5\n",
            "        unmatched_threshold: 0.5\n",
            "        ignore_thresholds: false\n",
            "        negatives_lower_than_unmatched: true\n",
            "        force_match_for_each_row: true\n",
            "        use_matmul_gather: true\n",
            "      }\n",
            "    }\n",
            "    similarity_calculator {\n",
            "      iou_similarity {\n",
            "      }\n",
            "    }\n",
            "    box_predictor {\n",
            "      weight_shared_convolutional_box_predictor {\n",
            "        conv_hyperparams {\n",
            "          regularizer {\n",
            "            l2_regularizer {\n",
            "              weight: 4e-05\n",
            "            }\n",
            "          }\n",
            "          initializer {\n",
            "            random_normal_initializer {\n",
            "              mean: 0.0\n",
            "              stddev: 0.01\n",
            "            }\n",
            "          }\n",
            "          activation: SWISH\n",
            "          batch_norm {\n",
            "            decay: 0.99\n",
            "            scale: true\n",
            "            epsilon: 0.001\n",
            "          }\n",
            "          force_use_bias: true\n",
            "        }\n",
            "        depth: 64\n",
            "        num_layers_before_predictor: 3\n",
            "        kernel_size: 3\n",
            "        class_prediction_bias_init: -4.6\n",
            "        use_depthwise: true\n",
            "      }\n",
            "    }\n",
            "    anchor_generator {\n",
            "      multiscale_anchor_generator {\n",
            "        min_level: 3\n",
            "        max_level: 7\n",
            "        anchor_scale: 4.0\n",
            "        aspect_ratios: 1.0\n",
            "        aspect_ratios: 2.0\n",
            "        aspect_ratios: 0.5\n",
            "        scales_per_octave: 3\n",
            "      }\n",
            "    }\n",
            "    post_processing {\n",
            "      batch_non_max_suppression {\n",
            "        score_threshold: 1e-08\n",
            "        iou_threshold: 0.5\n",
            "        max_detections_per_class: 100\n",
            "        max_total_detections: 100\n",
            "      }\n",
            "      score_converter: SIGMOID\n",
            "    }\n",
            "    normalize_loss_by_num_matches: true\n",
            "    loss {\n",
            "      localization_loss {\n",
            "        weighted_smooth_l1 {\n",
            "        }\n",
            "      }\n",
            "      classification_loss {\n",
            "        weighted_sigmoid_focal {\n",
            "          gamma: 1.5\n",
            "          alpha: 0.25\n",
            "        }\n",
            "      }\n",
            "      classification_weight: 1.0\n",
            "      localization_weight: 1.0\n",
            "    }\n",
            "    encode_background_as_zeros: true\n",
            "    normalize_loc_loss_by_codesize: true\n",
            "    inplace_batchnorm_update: true\n",
            "    freeze_batchnorm: false\n",
            "    add_background_class: false\n",
            "  }\n",
            "}\n",
            "train_config {\n",
            "  batch_size: 16\n",
            "  data_augmentation_options {\n",
            "    random_horizontal_flip {\n",
            "    }\n",
            "  }\n",
            "  data_augmentation_options {\n",
            "    random_scale_crop_and_pad_to_square {\n",
            "      output_size: 512\n",
            "      scale_min: 0.1\n",
            "      scale_max: 2.0\n",
            "    }\n",
            "  }\n",
            "  sync_replicas: true\n",
            "  optimizer {\n",
            "    momentum_optimizer {\n",
            "      learning_rate {\n",
            "        cosine_decay_learning_rate {\n",
            "          learning_rate_base: 0.08\n",
            "          total_steps: 300000\n",
            "          warmup_learning_rate: 0.001\n",
            "          warmup_steps: 2500\n",
            "        }\n",
            "      }\n",
            "      momentum_optimizer_value: 0.9\n",
            "    }\n",
            "    use_moving_average: false\n",
            "  }\n",
            "  fine_tune_checkpoint: \"./object_detection/models/checkpoints/detection/efficientdet_d0_coco17_tpu-32/checkpoint/ckpt-0\"\n",
            "  num_steps: 300000\n",
            "  startup_delay_steps: 0.0\n",
            "  replicas_to_aggregate: 8\n",
            "  max_number_of_boxes: 100\n",
            "  unpad_groundtruth_tensors: false\n",
            "  fine_tune_checkpoint_type: \"detection\"\n",
            "  use_bfloat16: true\n",
            "  fine_tune_checkpoint_version: V2\n",
            "}\n",
            "train_input_reader {\n",
            "  label_map_path: \"/content/tf_records/construction_label_map.pbtxt\"\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/tf_records/train.rcd-?????-of-?????\"\n",
            "  }\n",
            "}\n",
            "eval_config {\n",
            "  num_visualizations: 20\n",
            "  max_evals: 1\n",
            "  metrics_set: \"coco_detection_metrics\"\n",
            "  use_moving_averages: false\n",
            "  include_metrics_per_category: true\n",
            "  batch_size: 1\n",
            "  all_metrics_per_category: true\n",
            "}\n",
            "eval_input_reader {\n",
            "  label_map_path: \"/content/tf_records/construction_label_map.pbtxt\"\n",
            "  shuffle: false\n",
            "  num_epochs: 1\n",
            "  num_readers: 8\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/tf_records/test.rcd-?????-of-?????\"\n",
            "  }\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imw53mt5hGha"
      },
      "source": [
        "!python3 model_main_tf2.py \\\n",
        "--model_dir=trained_model \\\n",
        "--pipeline_config_path=efficientdet_d0_coco17_tpu-32/pipeline.config \\\n",
        "--alsologtostderr \\"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ2wTxLChIDA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aff6532-6504-4b31-9ac6-33e1d62244ec"
      },
      "source": [
        "cd ../"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy-ctzuCrS9s"
      },
      "source": [
        "## Adding Allegro Trains to TF2 OD API\n",
        "\n",
        "After we've installed [Allegro Trains](https://allegro.ai/trains-open-source/?utm_source=i_blog&utm_medium=referral&utm_campaign=trains_c)  using `pip install trains`, it's pretty straightforward to start using it within the TF2 OD API. For detailed documentation on Allegro Trains and all its functionalities take a look [here](https://allegro.ai/docs/). \n",
        "\n",
        "\n",
        "In the `model_main_tf2.py` script we've added these lines:\n",
        "\n",
        "1. `from trains import Task` - import Task class from trains\n",
        "\n",
        "\n",
        "\n",
        "> The Task class is the Trains Python Client package multipurpose class which supports experimentation and various workflows. In experimentation, a Task object connects your experiment code to Trains Server, where Trains stores it. All the parts of an experiment connect to a Task. This includes models, hyperparameters, and logging. A Task is, effectively, an experiment in Trains. Once it is stored in Trains Server, you can rerun the Task (experiment), reproduce it, and tune it.\n",
        "\n",
        "\n",
        "2. `task = Task.init(project_name=\"TF2 OD API - Evaluation of EfficientDet\", task_name=PurePath(FLAGS.pipeline_config_path).parent.name)` - define the project name and task name that will be shown in the Trains dashboard\n",
        "\n",
        "\n",
        "\n",
        "> Trains Automatically Logs Everything - with only two lines of code, you're getting:\n",
        "* Git repository, branch, commit id, entry point and local git diff\n",
        "* Python environment (including specific packages & versions)\n",
        "* stdout and stderr\n",
        "* Resource Monitoring (CPU/GPU utilization, temperature, IO, network, etc.)\n",
        "* Hyper-parameters\n",
        "    * ArgParser for command line parameters with currently used values\n",
        "    * Explicit parameters dictionary\n",
        "    * Tensorflow Defines (absl-py)\n",
        "* Initial model weights file\n",
        "* Model snapshots (With optional automatic upload to central storage: Shared folder, S3, GS, Azure, Http)\n",
        "* Artifacts log & store (Shared folder, S3, GS, Azure, Http)\n",
        "* Tensorboard/TensorboardX scalars, metrics, histograms, **images, audio and video**\n",
        "* [Matplotlib & Seaborn](https://github.com/allegroai/trains/tree/master/examples/frameworks/matplotlib)\n",
        "* Supported frameworks: [PyTorch](https://github.com/allegroai/trains/tree/master/examples/frameworks/pytorch), [Tensorflow](https://github.com/allegroai/trains/tree/master/examples/frameworks/tensorflow), [Keras](https://github.com/allegroai/trains/tree/master/examples/frameworks/keras), [AutoKeras](https://github.com/allegroai/trains/tree/master/examples/frameworks/autokeras), [XGBoost](https://github.com/allegroai/trains/tree/master/examples/frameworks/xgboost) and [Scikit-Learn](https://github.com/allegroai/trains/tree/master/examples/frameworks/scikit-learn) (MxNet is coming soon)\n",
        "* Seamless integration (including version control) with **Jupyter Notebook**\n",
        "    and [*PyCharm* remote debugging](https://github.com/allegroai/trains-pycharm-plugin)    \n",
        "\n",
        "\n",
        "\n",
        "3. `task.connect_configuration(FLAGS.pipeline_config_path)` - OPTIONAL: connect `pipeline.config` file so it's logged and shown in the Trains dashboard\n",
        "\n",
        "and after running the training script, we got all the eval experiments logged in the Trains dashboard at https://demoapp.trains.allegro.ai under TF2 OD API - Training Tutorial.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwVn0yKsAKdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e63f141-f667-4542-b40f-a8d10bd96deb"
      },
      "source": [
        "!pwd\n",
        "!ls /content/\n",
        "%cd /content/models/research\n",
        "\n",
        "import random\n",
        "import string\n",
        "\n",
        "PIPELINE_CONFIG_PATH='./object_detection/models/checkpoints/detection/efficientdet_d0_coco17_tpu-32/pipeline.config'\n",
        "NUM_TRAIN_STEPS=20000\n",
        "CHECKPOINT_EVERY_N=250\n",
        "EVAL_EVERY_N_SECONDS=2000\n",
        "SAMPLE_1_OF_N_EVAL_EXAMPLES=1\n",
        "PROJECT_NAME='\\TF2\\ OD\\ API\\ -\\ Training\\ Tutorial'\n",
        "TASK_NAME=''.join(random.choices(string.ascii_lowercase + string.digits, k=8))\n",
        "MODEL_DIR='/tmp/model_outputs_'+TASK_NAME\n",
        "\n",
        "\n",
        "print(\"Please find your results at https://demoapp.trains.allegro.ai with \\nPROJECT_NAME: \" + PROJECT_NAME.replace(\"\\\\\", \"\") + \"\\nTASK_NAME: \" + TASK_NAME)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "sample_data\n",
            "[Errno 2] No such file or directory: '/content/models/research'\n",
            "/content\n",
            "Please find your results at https://demoapp.trains.allegro.ai with \n",
            "PROJECT_NAME: TF2 OD API - Training Tutorial\n",
            "TASK_NAME: ovyr0xl3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9t8Pd__Sndoa",
        "outputId": "36a4bdef-d16c-4dda-c26e-10616324cb25"
      },
      "source": [
        "!python ./object_detection/model_main_tf2_train_eval.py --model_dir=$MODEL_DIR --num_train_steps=$NUM_TRAIN_STEPS --sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES --pipeline_config_path=$PIPELINE_CONFIG_PATH --project_name=$PROJECT_NAME --task_name=$TASK_NAME --eval_time=$EVAL_EVERY_N_SECONDS --checkpoint_every_n=$CHECKPOINT_EVERY_N"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python3: can't open file './object_detection/model_main_tf2_train_eval.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQdEbZMtkgqr"
      },
      "source": [
        "# Testing the trained model on sample images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sQLM_dRLfCja"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import io\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.builders import model_builder\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "def load_image_into_numpy_array(path):\n",
        "  \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "  Puts image into numpy array to feed into tensorflow graph.\n",
        "  Note that by convention we put it into a numpy array with shape\n",
        "  (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "  Args:\n",
        "    path: the file path to the image\n",
        "\n",
        "  Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "  \"\"\"\n",
        "  img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "  image = Image.open(BytesIO(img_data))\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (im_height, im_width, 3)).astype(np.uint8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CSBEkjr2fbBE"
      },
      "source": [
        "pipeline_config = PIPELINE_CONFIG_PATH\n",
        "model_dir = MODEL_DIR\n",
        "\n",
        "# Load pipeline config and build a detection model\n",
        "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
        "model_config = configs['model']\n",
        "detection_model = model_builder.build(\n",
        "      model_config=model_config, is_training=False)\n",
        "\n",
        "for latest_checkpoint in tf.train.checkpoints_iterator(model_dir, timeout=1):\n",
        "  ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
        "\n",
        "ckpt.restore(latest_checkpoint).expect_partial()\n",
        "\n",
        "def get_model_detection_function(model):\n",
        "  \"\"\"Get a tf.function for detection.\"\"\"\n",
        "\n",
        "  @tf.function\n",
        "  def detect_fn(image):\n",
        "    \"\"\"Detect objects in image.\"\"\"\n",
        "\n",
        "    image, shapes = model.preprocess(image)\n",
        "    prediction_dict = model.predict(image, shapes)\n",
        "    detections = model.postprocess(prediction_dict, shapes)\n",
        "\n",
        "    return detections, prediction_dict, tf.reshape(shapes, [-1])\n",
        "\n",
        "  return detect_fn\n",
        "\n",
        "detect_fn = get_model_detection_function(detection_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HX_AgEmZfjSr"
      },
      "source": [
        "label_map_path = configs['eval_input_config'].label_map_path\n",
        "label_map = label_map_util.load_labelmap(label_map_path)\n",
        "categories = label_map_util.convert_label_map_to_categories(\n",
        "    label_map,\n",
        "    max_num_classes=label_map_util.get_max_label_map_index(label_map),\n",
        "    use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "label_map_dict = label_map_util.get_label_map_dict(label_map, use_display_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOkYY-P-flly"
      },
      "source": [
        "!curl \"https://images.pexels.com/photos/3680959/pexels-photo-3680959.jpeg?crop=entropy&cs=srgb&dl=pexels-aleksey-3680959.jpg&fit=crop&fm=jpg&h=426&w=640\" --output \"/content/models/research/object_detection/test_images/construction2.jpg\"\n",
        "image_dir = '/content/models/research/object_detection/test_images/'\n",
        "image_path = os.path.join(image_dir, 'construction2.jpg')\n",
        "image_np = load_image_into_numpy_array(image_path)\n",
        "\n",
        "\n",
        "input_tensor = tf.convert_to_tensor(\n",
        "    np.expand_dims(image_np, 0), dtype=tf.float32)\n",
        "detections, predictions_dict, shapes = detect_fn(input_tensor)\n",
        "\n",
        "label_id_offset = 1\n",
        "image_np_with_detections = image_np.copy()\n",
        "\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np_with_detections,\n",
        "      detections['detection_boxes'][0].numpy(),\n",
        "      (detections['detection_classes'][0].numpy() + label_id_offset).astype(int),\n",
        "      detections['detection_scores'][0].numpy(),\n",
        "      category_index,\n",
        "      use_normalized_coordinates=True,\n",
        "      max_boxes_to_draw=20,\n",
        "      min_score_thresh=.80)\n",
        "\n",
        "plt.figure(figsize=(12,16))\n",
        "plt.imshow(image_np_with_detections)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}